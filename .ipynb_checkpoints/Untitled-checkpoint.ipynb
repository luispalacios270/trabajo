{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones requeridas del programa\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizarFrase(frase):    \n",
    "    archivoTemporalConFrase = open('temporalFile.txt','w')    \n",
    "    archivoTemporalConFrase.write(frase)\n",
    "    archivoTemporalConFrase.close()\n",
    "\n",
    "    file = {'file': open('temporalFile.txt', 'r')}\n",
    "    params = {'outf': 'tagged', 'format': 'json'}\n",
    "\n",
    "    url = \"http://www.corpus.unam.mx/servicio-freeling/analyze.php\"\n",
    "    r = requests.post(url, files=file, params=params)\n",
    "    obj = r.json()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenerDictionaryDeSenticol():\n",
    "    archivoDeBaseDeDatos = open('MLSenticon.txt','r',  encoding='cp1252')\n",
    "    palabras = archivoDeBaseDeDatos.read().split('\\n')\n",
    "    diccionarioDePalabrasConSentimientos = {}    \n",
    "    for palabra in palabras:\n",
    "        tmpPalabraConSentimiento = palabra.split('\\t')\n",
    "        diccionarioDePalabrasConSentimientos[tmpPalabraConSentimiento[0]] = tmpPalabraConSentimiento[1]\n",
    "    return diccionarioDePalabrasConSentimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leerBaseDeDatosDeReviews():\n",
    "    archivoDeBaseDeDatos = open('database.txt','r')\n",
    "    reviewsData = archivoDeBaseDeDatos.read().split('\\n')\n",
    "    archivoDeBaseDeDatos.close()\n",
    "    reviews, tipoDeReview = [],[] \n",
    "    for reviewData in reviewsData:\n",
    "        temporalData = reviewData.split('\\t')\n",
    "        reviews.append(temporalData[0])\n",
    "        tipoDeReview.append(temporalData[1])\n",
    "    return reviews, tipoDeReview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacionVectorial(reviewTokenizado, diccionarioDeSentimientos):    \n",
    "    representacion = [0, 0]\n",
    "    palabrasDeNegacion = ['pero', 'no']\n",
    "    finalizacionDeFrase = ['.', ',']\n",
    "    signoParaMultiplicarLasPalabras = 1\n",
    "    representacionTemporal = [0, 0]\n",
    "    \n",
    "    \n",
    "    for sentence in reviewTokenizado: \n",
    "        for word in sentence:\n",
    "            lemma = word['lemma']\n",
    "            if (lemma in finalizacionDeFrase):\n",
    "                representacion[0] += representacionTemporal[0] if (signoParaMultiplicarLasPalabras > 0) else representacionTemporal[1]\n",
    "                representacion[1] += representacionTemporal[1] if (signoParaMultiplicarLasPalabras > 0) else representacionTemporal[0]\n",
    "                representacionTemporal = [0, 0]\n",
    "            elif (lemma in palabrasDeNegacion):\n",
    "                signoParaMultiplicarLasPalabras = signoParaMultiplicarLasPalabras * (- 1);\n",
    "            elif (word['tag'][0] == 'A' and lemma in diccionarioDeSentimientos.keys()):\n",
    "                representacionTemporal[0] = representacionTemporal[0] + (float(diccionarioDeSentimientos[lemma]) if (float(diccionarioDeSentimientos[lemma]) > 0) else 0)\n",
    "                representacionTemporal[1] = representacionTemporal[1] + (float(diccionarioDeSentimientos[lemma]) if (float(diccionarioDeSentimientos[lemma]) < 0) else 0)\n",
    "            representacion[0] += representacionTemporal[0] if (signoParaMultiplicarLasPalabras > 0) else representacionTemporal[1]\n",
    "            representacion[1] += representacionTemporal[1] if (signoParaMultiplicarLasPalabras > 0) else representacionTemporal[0]\n",
    "            representacionTemporal = [0, 0]\n",
    "    representacion[1] *= -1 if (representacion[1] < 0) else 1\n",
    "    representacion[0] *= -1 if (representacion[0] < 0) else 1\n",
    "\n",
    "    return representacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_measures(Yestimado, Yteorico):\n",
    "    \n",
    "    CM = confusion_matrix(Yteorico, Yestimado)\n",
    "\n",
    "    TN = CM[0][0]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1]\n",
    "    FP = CM[0][1]\n",
    "    \n",
    "    sens = TP/(TP+FN)\n",
    "    esp = TN/(TN+FP)\n",
    "    \n",
    "    return sens, esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresionLogistica():\n",
    "\n",
    "\n",
    "lr=LogisticRegression()\n",
    "acc = []\n",
    "sens = []\n",
    "esp = []\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    Xtrain,Xtest,Ytrain,Ytest = train_test_split(bow, y)   #Realiza una única partición\n",
    "                                                          #de la base de datos.\n",
    "    \n",
    "    lr.fit(Xtrain,Ytrain)\n",
    "    Yest = lr.predict(Xtest)\n",
    "    s, e = error_measures(Yest,Ytest)\n",
    "    sens.append(s); esp.append(e)\n",
    "    acc.append(lr.score(Xtest,Ytest))\n",
    "\n",
    "print(\"Muestras training: \", round((np.size(Xtrain,0)*100)/205), \"%\")\n",
    "print(\"Muestras testing: \", round((np.size(Xtest,0)*100)/205), \"%\")\n",
    "    \n",
    "print(\"\\nResultados con Regresión logística (Lineal)\\n\")\n",
    "print(\"Accuracy: \", np.mean(acc), \"+/-\", np.std(acc))\n",
    "print(\"Sensitivity: \", np.mean(sens), \"+/-\", np.std(sens))\n",
    "print(\"Especificity: \", np.mean(esp), \"+/-\", np.std(esp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.375, 0]\n",
      "[0, 0.393]\n",
      "[0.75, 0]\n",
      "[0.9590000000000001, 0]\n",
      "[1.157, 0]\n",
      "[1.2189999999999999, 0]\n",
      "[1.2530000000000001, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[1.614, 0]\n",
      "[0, 0]\n",
      "[0.75, 0]\n",
      "[0.688, 0]\n",
      "[1.5939999999999999, 0]\n",
      "[1.3439999999999999, 0]\n",
      "[0, 0]\n",
      "[1.364, 0]\n",
      "[0, 0]\n",
      "[1.907, 0]\n",
      "[0, 0]\n",
      "[1.3439999999999999, 0]\n",
      "[0.5, 0]\n",
      "[0, 0]\n",
      "[0.661, 0]\n",
      "[0.338, 0]\n",
      "[0.354, 0]\n",
      "[1.5599999999999998, 0]\n",
      "[0.458, 0]\n",
      "[0.688, 0]\n",
      "[0.75, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0.281]\n",
      "[0.375, 0]\n",
      "[1.677, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[1.657, 0]\n",
      "[1.125, 0]\n",
      "[1.375, 0]\n",
      "[0.425, 0]\n",
      "[0.25, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0.25, 0]\n",
      "[0.75, 0]\n",
      "[1.354, 0]\n",
      "[0.625, 0]\n",
      "[1.5939999999999999, 0]\n",
      "[0, 0]\n",
      "[0.425, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0.375, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[1.0, 0]\n",
      "[0.25, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0.5]\n",
      "[0, 0]\n",
      "[0, 0.688]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0.406, 0]\n",
      "[0, 0]\n",
      "[0.25, 0]\n",
      "[0.656, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0.667]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 1.343]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0.604]\n",
      "[0, 0.375]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[1.151, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0, 0]\n",
      "[0.333, 0]\n",
      "[0, 0]\n",
      "[0, 0.292]\n",
      "[0, 0]\n",
      "[0, 0.5]\n",
      "[1.0310000000000001, 0]\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "diccionarioDeSentimientos = obtenerDictionaryDeSenticol()\n",
    "data, target = leerBaseDeDatosDeReviews()\n",
    "\n",
    "for review in data:\n",
    "    print(representacionVectorial(tokenizarFrase(review), diccionarioDeSentimientos))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
